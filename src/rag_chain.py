import logging
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from src.vector_store import load_vector_db
from src.config import Config

# 1. LOGGING AYARLARI
log_dir = "logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "rag_chain.log"), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def get_rag_chain():
    """
    RAG (Retrieval-Augmented Generation) zincirini oluÅŸturur.
    Bu zincir: Soru -> VektÃ¶r DB'de Ara -> BulunanlarÄ± LLM'e Ver -> Cevap Ãœret
    akÄ±ÅŸÄ±nÄ± yÃ¶netir.
    """
    try:
        # A) HafÄ±zayÄ± YÃ¼kle
        logger.info("ğŸ§  HafÄ±za (VektÃ¶r DB) yÃ¼kleniyor...")
        vector_store = load_vector_db()
        
        if not vector_store:
            raise Exception("VektÃ¶r veritabanÄ± yÃ¼klenemedi!")
            
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        # B) ZekayÄ± HazÄ±rla
        logger.info(f"ğŸ¤– Yapay Zeka Modeli hazÄ±rlanÄ±yor: {Config.LLM_MODEL_NAME}")
        llm = ChatGoogleGenerativeAI(
            model=Config.LLM_MODEL_NAME,
            google_api_key=Config.GOOGLE_API_KEY,
            temperature=0.3  
        )

        # C) TalimatlarÄ± HazÄ±rla.
        prompt_template = """
        Sen kurumsal bir asistansÄ±n. AÅŸaÄŸÄ±daki baÄŸlamÄ± (context) kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
        EÄŸer sorunun cevabÄ± baÄŸlamda yoksa, "Bu konuda bilgim yok" de, uydurma cevap verme.
        CevabÄ± verirken nazik ve profesyonel ol.

        BaÄŸlam (Context):
        {context}

        Soru:
        {question}

        Cevap:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )

        # D) Zinciri (Chain) Kur
        logger.info("ğŸ”— RAG Zinciri (Chain) oluÅŸturuluyor...")
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff", 
            retriever=retriever,
            return_source_documents=True, 
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        logger.info("âœ… RAG Zinciri hazÄ±r!")
        return chain

    except Exception as e:
        logger.error(f"âŒ RAG Zinciri oluÅŸturulurken hata: {e}")
        return None

if __name__ == "__main__":
    # TEST SENARYOSU
    logger.info("ğŸ§ª --- CHATBOT TESTÄ° BAÅLIYOR ---")
    
    qa_chain = get_rag_chain()
    
    if qa_chain:
        soru = "Bu dokÃ¼man ne hakkÄ±nda?" 
        
        logger.info(f"â“ Soru: {soru}")
        
        # Zinciri Ã‡alÄ±ÅŸtÄ±r
        response = qa_chain.invoke({"query": soru})
        
        print("\n" + "="*50)
        print(f"ğŸ¤– CEVAP:\n{response['result']}")
        print("="*50 + "\n")
        
        # KaynaklarÄ± GÃ¶ster 
        print("ğŸ“š Kaynaklar:")
        for doc in response['source_documents']:
            print(f"- {doc.metadata.get('source', 'Bilinmeyen Kaynak')}")
            
    logger.info("ğŸ --- TEST BÄ°TÄ°ÅÄ° ---")